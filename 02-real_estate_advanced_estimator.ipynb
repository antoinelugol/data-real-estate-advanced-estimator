{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Real Estate - advanced estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è In the previous challenge, we saw that if we have more flats than features  in our dataset ($\\large n$ observations $\\large> p$ features), that's why we can't \"solve\" the equation $\\large X\\theta = Y$. And without a deterministic formula for $\\large \\theta$, we would no longer be able to predict the prices of new flats anymore...!\n",
    "\n",
    "----\n",
    "\n",
    "üéØ In this exercise, we have now access to a bigger dataset consisting of 1000 flats and we want to refine our prediction for the same new flat as before:\n",
    "\n",
    "- `Surface`: 3000 $ft^2$\n",
    "- `Bedrooms`: 5 \n",
    "- `Floors`: 1\n",
    "\n",
    "‚ùå Instead of solving $\\large X\\theta = Y$ with a matrix $\\large X$ of shape $ (1000,4)$ that is **`non-invertible`**...\n",
    "\n",
    "üöÄ ...We will find $\\large {\\theta} = \\begin{bmatrix}\n",
    "     \\theta_0 \\\\\n",
    "     \\theta_1 \\\\\n",
    "    \\theta_2 \\\\\n",
    "     \\theta_3\n",
    "\\end{bmatrix}_{4 \\times 1}$ that minimizes the error $ \\large e = X\\hat{\\theta} - Y $: this approach is called a **`Linear Regression model`**. We will measure this error $e$ using the Euclidian distance $\\large \\left\\|e\\right\\|$ and the **`Mean Squared Error.`**\n",
    "\n",
    "üëâ Let's compute $\\large \\hat{\\theta}$ to find an approximate estimation of the new flat's price.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some \"default\" libraries\n",
    "# You can now use Pandas to manipulate the Dataframe conveniently\n",
    "\n",
    "''' Data manipulation'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "''' Data visualization'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the dataset **flats.csv** below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flats = pd.read_csv('https://wagon-public-datasets.s3.amazonaws.com/Machine%20Learning%20Datasets/flats.csv')\n",
    "flats.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÄ Use `scatterplots` to  figure out visually <u><i>which feature gives the most information about prices:</i></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ It seems that `surface` is a stronger indicator of price than the number of bedrooms or floors. In statistics, we say that `price` is more **correlated** with `surface` than with other features. \n",
    "\n",
    "üëâ Let's double-check this by running [`pandas.DataFrame.corr`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html) below, which computes correlation coefficients between each pair of columns of the DataFrame. \n",
    "\n",
    "<i> <u>Remarkable values:</u></i>\n",
    "* 1 means that the two columns  are perfectly  correlated üìà \n",
    "* -1 means that the two columns perfectly inversely correlated üìâ \n",
    "* 0 means that the two columns not *linearly* correlated üòê\n",
    "    \n",
    "<details>\n",
    "    <summary><i>Why do we use the correlation coefficient and not the covariance coefficient ?</i></summary>\n",
    "    \n",
    "‚úÖ <u>Similitudes</u>:\n",
    "    \n",
    "* üìà Positive correlations and positive covariances between two variables X and Y mean the same thing: When X increases, Y increases and vice-versa. When X decreases, Y decreases and vice-versa.\n",
    "    \n",
    "* üìâ Negative correlations and negative covariances between two variables X and Y mean the same thing: When X increases, Y decreases and vice-versa. When X decreases, Y increases and vice-versa\n",
    "    \n",
    "* ü§î A null correlation and a null covariance between two variables X and Y mean the same thing: \n",
    "    * They are _not linearly correlated_ in a sense that there would exists two real numbers $a$ and $b$  such that $ Y = aX + b$ \n",
    "    * However, they can still have another type of relation such as $Y = X^{2}$ (quadratic relation), $Y = e^{X}$ (exponential relation), $Y = ln(X)$ (logarithmic relation), $Y = sin(\\sqrt{1+X^7})$ (super weird relation), ...\n",
    "    \n",
    "‚ùóÔ∏è <u>Main difference</u>:\n",
    "    \n",
    "* üòñ The covariance between two variables X and Y can be infinitely big positively or negatively : $ cov(X,Y) \\in ( - \\infty ; + \\infty ) $\n",
    "    \n",
    "    * _Example_: if $ cov (X,Y) = 10 $ and $ cov (X,Z) = 30 $, can you say that X and Z are \"more correlated\" ? ‚ùå_Hell No !_ \n",
    "    * You cannot compare apples, oranges and bananas. \n",
    "\n",
    "* üßëüèª‚Äçüè´ How to solve this problem ?\n",
    "    * Consider the correlation instead, often denoted by the Greek Letter $\\rho$ (pronounce `\"rh√¥\"`)\n",
    "    \n",
    "üëâ<u>Consequence</u>:\n",
    "  \n",
    "* You can view the correlation as a _standardized covariance_, we simply divide the covariance by both the standard deviation of $X$ and the standard deviation of $Y$    \n",
    "$$ \\large  \\rho(X,Y) = \\frac{cov(X,Y)}{\\sigma_X \\sigma_Y} \\in [0;1]$$\n",
    "    \n",
    "    *  _Let's continue our example_: suppose now that $ \\rho(X,Y) = 0.80 $ and $ \\rho(X,Z) = 0.15 $, would you still want to say that X and Z are more correlated ? No. Actually, X and Y are more correlated than X and Z !\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üé® For a quicker glimpse of this matrix, you can use a **heatmap** from [`seaborn.heatmap`](https://seaborn.pydata.org/generated/seaborn.heatmap.html)\n",
    "\n",
    "<details>\n",
    "    <summary><i>Additional tips to display a nicer correlation matrix</i></summary>\n",
    "\n",
    "* **`cmap`**: *Seaborn* being a visualisation library built on top of *Matpotlib*, you can use the argument [*cmap*](https://matplotlib.org/stable/tutorials/colors/colormaps.html) which stands for _color map_ to use \n",
    "\n",
    "* **`annot`** : To read the correlations even faster, you can show the correlation coefficients directly on the colored heatmap\n",
    "   \n",
    "* **`annot_kws`** : You can customize how the correlation coefficients appear\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üß™ Test your code!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('flats',\n",
    "    shape=flats.shape,\n",
    "    columns=flats.columns\n",
    ")\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Estimator with 1 feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to build a statistical estimator of **price**  as a function of  only one feature, the **surface**.\n",
    "\n",
    "üéØ Let's try to fit a **linear regression** between the two variables.\n",
    "\n",
    "Practically speaking, we want to choose the best parameters $\\hat{\\theta}$ = (`slope`, `intercept`) such that the `predicted price = slope * surface + intercept` is as close as possible to the `price` in terms of Mean Squared Errors.\n",
    "\n",
    "üìÖ During the next weeks, we will discover and study different models (Linear Regression, KNN, Logistic Regression, Neural Networks, etc.). \n",
    "\n",
    "üëâ An important part of your job is to choose the right model and optimizing the parameters to make the best predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Visual approach"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Implement the function `plot_line(slope, intercept, ax)` \n",
    "* This function plots a line with the `slope` and `intercept` arguments on the `ax` figure. We added the argument ax, so that you can plot the line(s) on your scatterplot.\n",
    "* When you are done with coding the function, play with different values of `(slope, intercept)` until you find a ‚Äúgood linear approximation‚Äù of the data. Can you find the best fit ?\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_line(a, b, ax):\n",
    "    \"\"\"Plot a line from slope and intercept on the ax\"\"\"\n",
    "    pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: play with slopes and intercepts to get the best fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üòÖ Not so easy (and not very ‚Äúscientific‚Äù), right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) Computational approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üî• To make sure that our estimator line is the best possible one, we need to compute  the **Mean Squared Error** between the **real prices** and the **predicted prices**!\n",
    "\n",
    "üëâ Remember that :\n",
    "\n",
    "* For each appartment, `predicted_price = slope * surface + intercept`\n",
    "* Both the **vector of real prices** and the **vector of predicted prices** are of shape $ (1000,1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.2.1) Squared Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><b>Step 1</b></u>\n",
    "\n",
    "‚ùìFor each row (_i.e. flat_), we should evaluate the `squared_error = (price - predicted_price)**2` ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_errors(slope, intercept, surfaces, prices):\n",
    "    \"\"\"TO DO: return an array containing the squared errors between \n",
    "    all real prices from the dataset and the predicted prices\n",
    "    \"\"\"\n",
    "    pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° A general principle in Data Science / Modelling is that $ \\large error = f(y, \\hat{y})$ where:\n",
    "\n",
    "* $ \\large y$ is the real value\n",
    "* $ \\large \\hat{y} $ the predicted valute\n",
    "* $ \\large f$ is often called a **Loss Function** or a **Cost Function** \n",
    "    * üìÜ *Cf.* **`Machine Learning I > Model Tuning`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.2.2) Mean Squared Errors (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><b>Step 2</b></u>\n",
    "\n",
    "‚ùì Create the `mse` function which should return the mean of the array returned by the `squared_errors` function. ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(slope, intercept, surfaces, prices):\n",
    "    '''TO DO: Return the mean of the array contained in squared_errors as a float.'''\n",
    "    pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ In section _(2.1) Visual approach_, you visually tried to estimate the \"best line\", which consists in finding the best pair `(slope, intercept)`. \n",
    "\n",
    "‚ùìUsing this \"best pair\", compute the MSE of your estimator. ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3) Finding the best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Keep playing with different values for `slope` and `intercept` and try to get the best fit by hand!  Notice how hard it is to optimize both parameters at the same time.\n",
    "\n",
    "\n",
    "üëá Follow the steps down below to get an idea of one potential approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.3.1) Finding the `best slope`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i><u> Step 1 :</u></i></b>\n",
    "\n",
    "Start by fixing an  `initial_intercept` with your best estimate, then find a slope which approximately minimizes the function  `mse=f(slope)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an initial_intercept, for instance, we may suppose there is always a small transaction fee even for very small flats?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a range of 100 slopes which contains what you believe is the optimal slope (hint: np.linspace())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of MSEs for each slope value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Plot MSEs vs. slopes. Do you see a minimum ‚ùì\n",
    "\n",
    "üôÉ If not, try another range of slopes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Compute the minimum value of MSE for your `initial_intercept`, and the corresponding `slope_best` value ‚ùì\n",
    "<br>\n",
    "<details>\n",
    "    <summary><i>Hint</i></summary>\n",
    "    \n",
    "Here you can use Python's built-in `.min()` function, as well as `List.index()` method\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.3.2) Finding the `best intercept`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i><u> Step 2 :</u></i></b>\n",
    "\n",
    "üî® Let's now fix the slope to that `slope_best` value, then re-use the previous approach to find \"the\" `intercept_best`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a range of 100 intercepts which contains what you believe is the optimal intercept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of MSEs for each intercept value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MSEs against slopes. Do you see a minimum? If not, try another range of slopes!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Compute `mse_min`, the minimum value of MSEs when slope is equal to `slope_best` best, and store the corresponding best intercept as `intercept_best` ‚ùì\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_min = None\n",
    "intercept_best = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üß™ Test your code!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('univariate',\n",
    "    mse_min=mse_min,                \n",
    "    slope_best=slope_best,\n",
    "    intercept_best=intercept_best\n",
    ")\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ **Great job**! By adding a non-zero intercept parameter, we have been able to reduce the MSE even more (Feel free to plot the regression line in your scatter plot to  confirm \"visually\" the approximate fit).\n",
    "\n",
    "‚ùì However, what guarantees that these (`intercept_best`, `slope_best`) parameters are really the best ones? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>üëÄ Explanations</i></summary>\n",
    "    \n",
    "We could maybe find an even better slope value by repeating step ‚ë†, this time fixing intercept at `intercept_best`! And then repeat step ‚ë° with the new slope to adjust the intercept again...\n",
    "    \n",
    "In order to find the global minimum of a 2-parameter function RMSE = f(slope, intercept), we may have to repeat step ‚ë† and ‚ë° indefinitely until values converge towards absolute minimums - and with no guarantee of success.\n",
    "    \n",
    "<img src='https://wagon-public-datasets.s3.amazonaws.com/data-science-images/decision-science/real-estate-minimizer.png'>\n",
    "    \n",
    "üí™ You've just discovered one of the most fundamental aspects of machine learning: **iterative process for finding mimina**.  \n",
    "\n",
    "üëâ As you can guess, in the Data Scientist world, algorithms have been developed to automate and optimize such processes. In the next few weeks, you will discover the power of other algorithms such as **Gradient Descent**, and Python libaries such as `Stastmodels` that perform this iterative process for you. \n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Run the cells below if you are curious and want to find the real best slope and intercept for this dataset, computed using **Gradient Descent** (üìÜ covered in **Machine Learning I > Under The Hood**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try this out (Seaborn visual solution)\n",
    "sns.regplot(data=flats, x='surface', y='price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try this out (statsmodels, exact solution)!\n",
    "import statsmodels.formula.api as smf\n",
    "regression = smf.ols(formula= 'price ~ surface', data=flats).fit()\n",
    "\n",
    "print(\"intercept_best\", regression.params['Intercept'])\n",
    "print(\"slope_best\", regression.params['surface'])\n",
    "print('mse_best: ', np.mean(regression.resid**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Let's go back to our initial question: What is your new prediction for the 5th flat below? How does it compare with your initial prediction based only on 4 flats? \n",
    "\n",
    "- `surface`: 3000 $ft^2$\n",
    "- `bedrooms`: 5 \n",
    "- `floors`: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute predicted price (Remember that the real price is 750,000$)\n",
    "pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è It's better than our initial deterministic estimator based on only 4 flats, but obviously we are missing the information provided by the number of bedrooms and floors in this prediction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Estimator with _all features_ (surface, bedrooms, floors)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° A linear regression with three features (**Multivariate Linear Regression**) works the same way as with one feature, but instead of determining only 2 parameters to minimize RMSE (`intercept` and `slope`), we will need to find 4 parameters: $\\hat{\\theta}$ = (`intercept`, `slope_surface`, `slope_bedrooms`, `slope_floors`). \n",
    "\n",
    "----\n",
    "\n",
    "üóì There will be a lecture fully dedicated to **Multivariate Linear Regression** in the **Decision Science** module.\n",
    "\n",
    "----\n",
    "\n",
    "üóì The same **Gradient Descent** iterative method is applicable, and you will code it yourself by hand later during the bootcamp.\n",
    "\n",
    "üëâ Meanwhile, feel free to run the cell below to see the final result of this multivariate regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the four regression coefficients by running this cell\n",
    "import statsmodels.formula.api as smf\n",
    "regression = smf.ols(formula= 'price ~ surface + bedrooms + floors', data=flats).fit()\n",
    "regression.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ Now that we have found the best parameters $\\theta = \\begin{bmatrix}\n",
    "     \\theta_0 \\\\\n",
    "     \\theta_1 \\\\\n",
    "    \\theta_2 \\\\\n",
    "     \\theta_3 \\\\\n",
    "\\end{bmatrix}_{4 \\times 1} = \n",
    "\\begin{bmatrix}\n",
    "     \\theta_{intercept} \\\\\n",
    "     \\theta_{surface}\\\\\n",
    "    \\theta_{bedrooms} \\\\\n",
    "     \\theta_{floors}\n",
    "\\end{bmatrix}_{4 \\times 1} = \n",
    "\\begin{bmatrix}\n",
    "    18.154854 \\\\\n",
    "    0.286953 \\\\\n",
    "    -21.623564 \\\\\n",
    "    -3.811868\n",
    "\\end{bmatrix}_{4 \\times 1}\n",
    "$, \n",
    "\n",
    "we can predict the price of the new flat with:\n",
    "* $3000 ft^2$\n",
    "* $5$ bedrooms\n",
    "* located on the $1st$ floor\n",
    "\n",
    "$$ \\hat{y_5} = \\theta_0 + \\theta_1 \\times 3000 + \\theta_2 \\times 5 + \\theta_3 \\times 1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the newly predicted price for the 5th flat? Is this prediction better?\n",
    "pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Concluding remarks on Linear Algebra üß†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This **optimisation problem** can be summarized as follows\n",
    "\n",
    "- We need to find a vector of parameters $\\hat{\\theta} = \\begin{bmatrix}\n",
    "     \\theta_{intercept} \\\\\n",
    "     \\theta_{surface}\\\\\n",
    "    \\theta_{bedrooms} \\\\\n",
    "     \\theta_{floors}\n",
    "\\end{bmatrix}_{4 \\times 1}$ \n",
    "\n",
    "- That minimizes an error $e = \\left\\|X\\hat{\\theta} - Y \\right\\|^2$\n",
    "\n",
    "- For a given matrix of features $X$ [constant, surfaces, floors, bedrooms]  $\\begin{bmatrix}\n",
    "    1 & 620 & 1 & 1 \\\\\n",
    "    1 & 3280 & 4 & 2 \\\\\n",
    "    ... \\\\\n",
    "    1 & 1900 & 2 & 2 \\\\\n",
    "    1 & 1320 & 3 & 3\n",
    "   \\end{bmatrix}_{n \\times 4}\n",
    "$\n",
    "\n",
    "- and a vector of observations $Y = \\begin{bmatrix}\n",
    "           y_{1} \\\\\n",
    "           y_{2} \\\\\n",
    "           \\vdots \\\\\n",
    "           y_{1000}\n",
    "         \\end{bmatrix}$ (prices)\n",
    "\n",
    "Such minimum $\\large \\hat{\\theta}$ is reached when the \"derivatives\" of $e$, that is $ \\large 2 X^T(X\\hat{\\theta}‚àíY)$ equals zero (üëâproof during the Decision Module). \n",
    "    \n",
    "In other words, we need to solve the linear system $\\large (X^T X)\\hat{\\theta}=X^TY$. \n",
    "    \n",
    "This linear has a unique solution provided that no column of X can be expressed as a linear combination of the others: in that case $ \\large (X^TX)^{-1}$ is invertible and the minimum is reached when $\\large \\hat{\\theta} = (X^TX)^{-1} X^T Y$. Notice how X does not need to be squared anymore compared to the first challenge üí™.\n",
    "\n",
    "üí• However, keep in mind that ***inverting matrices is computationally complex***. That is why other methods have been developed to find the minimum of a function, such as ***gradient descent***.\n",
    "    \n",
    "üìö Read more on [Stats.StackExchange](https://stats.stackexchange.com/a/278779) if you are interested!\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ Congratulations! \n",
    "\n",
    "üíæ Don't forget to `git add/commit/push` your notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
